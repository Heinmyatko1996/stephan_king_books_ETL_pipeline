# ðŸ“š Stephen King Books ETL Pipeline

This is an end-to-end ETL pipeline project that extracts data from the **Google Books API** (focused on Stephen Kingâ€™s works), processes it using **Python**, and loads the data into a **PostgreSQL** database, all orchestrated via **Apache Airflow**. Optionally, data can be visualized in **Apache Superset**.

---

## ðŸ”§ Tech Stack

- ðŸ³ Docker
- â›“ï¸ Docker Compose
- ðŸ˜ PostgreSQL
- ðŸŒ€ Apache Airflow
- ðŸ” Google Books API
- ðŸ Python (`requests`, `json`, etc.)
- ðŸ“Š Apache Superset (optional)

---

## ðŸ“ Project Structure

```
stephan_king_books_ETL_pipeline/
â”œâ”€â”€ app.py                # Main ETL script (extract-transform-load)
â”œâ”€â”€ dags/                 # Airflow DAGs (if applicable)
â”œâ”€â”€ config/               # Configuration files
â”œâ”€â”€ plugins/              # Airflow plugins (optional)
â”œâ”€â”€ logs/                 # Airflow logs
â”œâ”€â”€ docker-compose.yml    # Superset + Postgres stack
â”œâ”€â”€ venv/                 # Local Python virtual environment (excluded from Docker)
â””â”€â”€ README.md             # This file
```

---

## ðŸš€ How It Works

1. **Extract**: Query the [Google Books API](https://developers.google.com/books) to retrieve books authored by Stephen King.
2. **Transform**: Clean and structure the JSON response.
3. **Load**: Insert structured data into a PostgreSQL database inside Docker.

---

## ðŸ› ï¸ Getting Started

### 1. Clone the Repository

```bash
git clone https://github.com/Heinmyatko1996/stephan_king_books_ETL_pipeline
cd stephan_king_books_ETL_pipeline
venv .venv
source venv/bin/activate
pip install -r requirements.txt
```

### 2. Run the Docker Stack

```bash
docker compose up -d
```

- PostgreSQL will run in a container with `stephan_king_books_etl_pipeline-postgres-1`.

---

## ðŸ Run ETL Script Manually (if virtual environment is already on, there is no need to run it manually, all data will be loaded)

Make sure your virtualenv is active, or install dependencies with:

```bash
pip install requests psycopg2-binary
```

Then run:

```bash
python app.py
```

> This script connects to the running Postgres container and loads data into a table (e.g., `books`).

---

## ðŸ§ª Example Output Table (PostgreSQL)

![image](https://github.com/user-attachments/assets/ac82f07e-a178-4ec4-a40b-25f9136aeccd)

---

## ðŸ›¬ Airflow

Data is already been loaded using airflow dag. See the app.py python script for invoking api and loading data into postgres table.
Postgres database name: google_books

schema name: public

table name: books

---

## ðŸ“š Data Source

- **Google Books API**  
  Docs: https://developers.google.com/books/docs/v1/using  
  Example Endpoint:  
  `https://www.googleapis.com/books/v1/volumes?q=inauthor:Stephen+King`

---

## ðŸ§  Key Learnings

- Dockerized development with multiple services
- ETL development and orchestration using Python
- API extraction and data normalization
- Integration between Airflow (or cron), PostgreSQL

---

## ðŸ“„ License

MIT License. See [LICENSE](LICENSE) file for details.

---

## ðŸ™Œ Acknowledgements

Thanks to Google Books API and Stephen King for being prolific enough to build pipelines on!
